# Week 4: Dynamic Programming

This week, you will learn how to compute value functions and optimal policies, assuming you have the MDP model. You will implement dynamic programming to compute value functions and optimal policies and understand the utility of dynamic programming for industrial applications and problems. Further, you will learn about Generalized Policy Iteration as a common template for constructing algorithms that maximize reward. For this weekâ€™s graded assessment, you will implement an efficient dynamic programming agent in a simulated industrial control problem.

## Key Concepts

- Understand the distinction between policy evaluation and control
- Explain the setting in which dynamic programming can be applied, as well as its limitations
- Outline the iterative policy evaluation algorithm for estimating state values under a given policy
- Apply iterative policy evaluation to compute value functions
- Understand the policy improvement theorem
- Use a value function for a policy to produce a better policy for a given MDP
- Outline the policy iteration algorithm for finding the optimal policy
- Understand "the dance of policy and value"
- Apply policy iteration to compute optimal policies and optimal value functions
- Understand the framework of generalized policy iteration
- Outline value iteration, an important example of generalized policy iteration
- Understand the distinction between synchronous and asynchronous dynamic programming methods
- Describe brute force search as an alternative method for searching for an optimal policy
- Describe Monte Carlo as an alternative method for learning a value function
- Understand the advantage of Dynamic programming and "bootstrapping" over these alternative strategies for finding the optimal policy
