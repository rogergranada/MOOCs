{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Using vectorization in Python\n",
    "\n",
    "This notebook demonstrates how to use vectorization in Python as well as comparisons of using vectorization and using for-loops."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "# import libraries\n",
    "import numpy as np\n",
    "import time\n",
    "\n",
    "HOW_LONG_TO_RUN = 100\n",
    "\n",
    "a = np.random.rand(1000000)\n",
    "b = np.random.rand(1000000)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Vectorized vs. For Loop"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "250047.14194216448\n",
      "Loop version: 553.812026978 ms\n"
     ]
    }
   ],
   "source": [
    "tic = time.time()\n",
    "c = 0\n",
    "for j in range(len(a)):\n",
    "    c += a[j] * b[j]\n",
    "toc = time.time()\n",
    "print(c)\n",
    "print(\"Loop version: \" + str(1000*(toc - tic)) + \" ms\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "250047.14194217467\n",
      "Loop version: 9.2921257019 ms\n"
     ]
    }
   ],
   "source": [
    "tic = time.time()\n",
    "c = np.dot(a, b)\n",
    "toc = time.time()\n",
    "print(c)\n",
    "print(\"Loop version: \" + str(1000*(toc - tic)) + \" ms\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# More Vectorization Examples"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def vectorized(a, b, nb_times):\n",
    "    m = []\n",
    "    for i in xrange(nb_times):\n",
    "        tic = time.time()\n",
    "        c = np.dot(a, b)\n",
    "        toc = time.time()\n",
    "        #print(\"Vectorized version: \" + str(1000*(toc - tic)) + \" ms\")\n",
    "        m.append(1000*(toc - tic))\n",
    "\n",
    "    mean_m = (np.array(m).sum())/float(nb_times)\n",
    "    return mean_m, c"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Mean of 100 runnings as vectorized version: 0.910415649414 ms\n",
      "Value: 250490.600103\n"
     ]
    }
   ],
   "source": [
    "mean_m, c = vectorized(a, b, HOW_LONG_TO_RUN)\n",
    "print(\"Mean of \"+ str(HOW_LONG_TO_RUN) +\" runnings as vectorized version: \"+ str(mean_m) +\" ms\")\n",
    "print(\"Value: \"+ str(c))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def for_loop_zip(a, b, nb_times):\n",
    "    m = []\n",
    "    c = 0\n",
    "    for i in xrange(nb_times):\n",
    "        tic = time.time()\n",
    "        for a_j, b_j in zip(a, b):\n",
    "            c += a_j * b_j\n",
    "        toc = time.time()\n",
    "        #print(\"Zip version: \" + str(1000*(toc - tic)) + \" ms\")\n",
    "        m.append(1000*(toc - tic))\n",
    "\n",
    "    mean_m = (np.array(m).sum())/float(nb_times)\n",
    "    return mean_m, c"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Mean of 100 runnings as for loop version: 406.943519115 ms\n",
      "Value: 25049060.0103\n"
     ]
    }
   ],
   "source": [
    "mean_m, c = for_loop_zip(a, b, HOW_LONG_TO_RUN)\n",
    "print(\"Mean of \"+ str(HOW_LONG_TO_RUN) +\" runnings as for loop version: \"+ str(mean_m) +\" ms\")\n",
    "print(\"Value: \"+ str(c))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def for_loop(a, b, nb_times):\n",
    "    m = []\n",
    "    c = 0\n",
    "    for i in xrange(nb_times):\n",
    "        tic = time.time()\n",
    "        for j in range(len(a)):\n",
    "                c += a[j] * b[j]\n",
    "        toc = time.time()\n",
    "        #print(\"Loop version: \" + str(1000*(toc - tic)) + \" ms\")\n",
    "        m.append(1000*(toc - tic))\n",
    "\n",
    "    mean_m = (np.array(m).sum())/float(nb_times)\n",
    "    return mean_m, c"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Mean of 100 runnings as for loop version: 349.924705029 ms\n",
      "Value: 25049060.0103\n"
     ]
    }
   ],
   "source": [
    "mean_m, c = for_loop(a, b, HOW_LONG_TO_RUN)\n",
    "print(\"Mean of \"+ str(HOW_LONG_TO_RUN) +\" runnings as for loop version: \"+ str(mean_m) +\" ms\")\n",
    "print(\"Value: \"+ str(c))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Other vectorizations in Python Numpy library"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "vector: [0.14923004 0.68556227 0.85590332 ... 0.6686893  0.52930779 0.32877505]\n",
      "exponential: [1.16094003 1.98488756 2.35349938 ... 1.95167758 1.69775669 1.38926531]\n",
      "log: [-1.90226624 -0.37751595 -0.15559785 ... -0.40243575 -0.63618519\n",
      " -1.1123815 ]\n",
      "abs: [0.14923004 0.68556227 0.85590332 ... 0.6686893  0.52930779 0.32877505]\n",
      "maximum: [0.24038818 0.68556227 0.85590332 ... 0.89155843 0.52930779 0.32877505]\n",
      "square: [0.02226961 0.46999562 0.73257049 ... 0.44714538 0.28016673 0.10809303]\n"
     ]
    }
   ],
   "source": [
    "print('vector: {}'.format(a))\n",
    "u = np.exp(a) # exponential\n",
    "print('exponential: {}'.format(u))\n",
    "v = np.log(a)\n",
    "print('log: {}'.format(v))\n",
    "w = np.abs(a)\n",
    "print('abs: {}'.format(w))\n",
    "x = np.maximum(a, b)\n",
    "print('maximum: {}'.format(x))\n",
    "y = a**2\n",
    "print('square: {}'.format(y))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "### Logistic Regression with vectors\n",
    "\n",
    "Calculating for $m$ examples and $n = 2$ features <font color='red'>without</font> vectorization\n",
    "\n",
    "$\n",
    "J = 0;\\ \\ dw_1 = 0;\\ \\ dw_2 = 0;\\ \\ db = 0 \\\\\n",
    "\\\\\n",
    "\\text{For}\\ \\ i=1\\ \\ \\text{to}\\ \\ m \\\\\n",
    "\\hspace{15pt}z^{(i)} = w^Tx^{(i)} + b \\\\ \n",
    "\\hspace{15pt}a^{(i)} = \\sigma(z^{(i)}) \\\\\n",
    "\\hspace{15pt}J\\ += - [y^{(i)}\\ log(a^{(i)}) + (1 - y^{(i)})\\ log(1 - a^{(i)})] \\\\\n",
    "\\hspace{15pt}dz^{(i)} = a^{(i)} - y^{(i)} \\\\\n",
    "\\hspace{15pt}\\text{For}\\ \\ j=1 \\ \\text{to} \\ \\ n \\\\\n",
    "\\hspace{30pt}dw_j += x_j^{(i)}dz^{(i)} \\\\\n",
    "\\hspace{15pt}\\text{End for} \\\\\n",
    "\\hspace{15pt}db += dz^{(i)} \\\\\n",
    "\\hspace{15pt}J\\ /=\\ m \\\\\n",
    "\\hspace{15pt}dw_1\\ /=\\ m \\\\\n",
    "\\hspace{15pt}dw_2\\ /=\\ m \\\\\n",
    "\\hspace{15pt}db\\ /=\\ m \\\\\n",
    "\\text{End for}\n",
    "$\n",
    "\n",
    "### Updating weights\n",
    "\n",
    "$\n",
    "w_1\\ := w_1 - \\alpha dw_1 \\\\\n",
    "w_2\\ := w_2 - \\alpha dw_2 \\\\\n",
    "b\\ := b - \\alpha db \\\\\n",
    "$\n",
    "\n",
    "The code in Python implementing this function is presented below:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 103,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Initialize elements\n",
    "def sigmoid(z):\n",
    "    return 1./(1. + np.exp(-1.*z))\n",
    "\n",
    "#hyperparameter\n",
    "lr = 0.1\n",
    "# input\n",
    "x = np.array([\n",
    "    [0.1, 0.1],\n",
    "    [0.2, 0.2],\n",
    "    [0.3, 0.3]\n",
    "])\n",
    "m = x.shape[0]\n",
    "y = np.array([1, 2, 3])\n",
    "# weights\n",
    "w = np.array([0.2, 0.3])\n",
    "n = w.shape[0]\n",
    "dw = np.zeros((n, 1))\n",
    "# bias\n",
    "b = np.array([1.0])\n",
    "db = np.zeros(b.shape)\n",
    "\n",
    "z = np.zeros((m, 1))\n",
    "dz = np.zeros((m, 1))\n",
    "a = np.zeros((m, 1))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 83,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Initial error: 0.0\n",
      "Error: -0.845841870694\n",
      "Initial weights: [0.2, 0.3]\n",
      "Updated weights: [0.23160057 0.33160057]\n"
     ]
    }
   ],
   "source": [
    "J, dw1, dw2, db = 0.0, 0.0, 0.0, 0.0\n",
    "for i in range(x.shape[0]):\n",
    "    z[i] = np.dot(w, x[i]) + b\n",
    "    a[i] = sigmoid(z[i])\n",
    "    J += -(y[i] * math.log(a[i]) + (1 - y[i]) * math.log(1 - a[i]))\n",
    "    dz[i] = a[i] - y[i]\n",
    "    for j in range(n):\n",
    "        dw[j] += x[i][j] * dz[i]\n",
    "    db += dz[i]\n",
    "J /= m\n",
    "for j in range(n):\n",
    "    dw[j] /= m\n",
    "db /= m\n",
    "    \n",
    "for j in range(n):\n",
    "    w[j] = w[j] - lr * dw[j]\n",
    "b = b - lr * db\n",
    "\n",
    "print('Initial error: 0.0')\n",
    "print('Error: {}'.format(J))\n",
    "print('Initial weights: [0.2, 0.3]')\n",
    "print('Updated weights: {}'.format(w))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Vectorizing Logistic Regression\n",
    "\n",
    "**Calculating for $m$ examples and $n = 2$ features <font color='red'>with</font> vectorization**\n",
    "\n",
    "Consider that we have `m` training examples. In training using for loops, we make a prediction on the first example by computing $Z^{(1)}$. Then compute the activations $a^{(1)}$ in this first example. Next, we make a prediction on the second training example by computing $Z^{(2)}$ and then computing the activations $a^{(2)}$. Next, we make a prediction on the third example, by computing $Z^{(3)}$ and then computing the activations $a^{(3)}$, and so on. And you might need to do this `m` times if you have `m` training examples.\n",
    "\n",
    "$$\n",
    "\\begin{matrix}\n",
    "z^{(1)} = w^Tx^{(1)}+b & \\ \\ \\ \\ \\ & z^{(2)} = w^Tx^{(2)}+b & \\ \\ \\ \\ \\ & z^{(3)} = w^Tx^{(3)}+b \\\\ \n",
    "a^{(1)} = \\sigma (z^{(1)}) & \\ \\ \\ \\ \\ & a^{(2)} = \\sigma (z^{(2)}) & \\ \\ \\ \\ \\ & a^{(3)} = \\sigma (z^{(3)}) \\\\\n",
    "\\end{matrix}\n",
    "$$\n",
    "\n",
    "Here, we define a matrix capital `X` to be your training inputs, stacked together in different columns as illustred below. This matrix is a ($n_x \\times m$) matrix. So, this means that `X` is a $\\mathbb{R}^{n_x,m}$ dimensional matrix. \n",
    "\n",
    "$\n",
    "X = \\begin{bmatrix}\n",
    "| & | &  & |\\\\\n",
    "x^{(1)} & x^{(2)} & \\dots & x^{(m)} \\\\\n",
    "| & | &  & |\\\\\n",
    "\\end{bmatrix} \\hspace{15pt}\\rightarrow\\hspace{15pt} (n_x, m)\\hspace{15pt} \\mathbb{R}^{n_x,m}\n",
    "$\n",
    "\n",
    "To compute $Z^{(1)}$, $Z^{(2)}$, $Z^{(3)}$ and so on, all in one step, we construct a ($1\\times M$) matrix (row vector) to compute them all at the same time. \n",
    "\n",
    "$\n",
    "Z = [ z^{(1)}\\ \\ z^{(2)}\\ \\ \\dots \\ \\ z^{(m)}] \\\\ \n",
    "Z = w^TX + \n",
    "\\begin{bmatrix}\n",
    "b_1 & b_2 & \\ldots & b_m\n",
    "\\end{bmatrix} \\hspace{15pt}\\rightarrow\\hspace{15pt} \\text{where}\\ \\ \\ b \\rightarrow (1,m) \\ \\ \\ \\text{i.e.,} \\ \\ \\  b \\in \\mathbb{R}^{1,m} \\ \\ \\ \\text{dimension} \\\\\n",
    "Z = np.dot(w^T, X) + b \\\\\n",
    "A = [ a^{(1)}\\ \\ a^{(2)}\\ \\ \\dots\\ \\ a^{(m)}] = \\sigma(Z)\n",
    "$\n",
    "\n",
    "Thus, we compute:\n",
    "\n",
    "$\n",
    "Z = \\begin{bmatrix}\n",
    "w_1 & w_2 & \\ldots & w_n\n",
    "\\end{bmatrix}\n",
    "\\begin{bmatrix}\n",
    "| & | &  & |\\\\\n",
    "x^{(1)} & x^{(2)} & \\dots & x^{(m)} \\\\\n",
    "| & | &  & |\\\\\n",
    "\\end{bmatrix} + \n",
    "\\begin{bmatrix}\n",
    "b_1 & b_2 & \\ldots & b_m\n",
    "\\end{bmatrix} \\\\\n",
    "Z = \\begin{matrix}\n",
    "[w^Tx^{(1)}+b_1 & w^Tx^{(2)}+b_2 & ...  & w^Tx^{(m)}+b_m] & \\hspace{15pt}\\rightarrow\\hspace{15pt} (1, m)\\hspace{15pt} \\mathbb{R}^{1,m}\n",
    "\\end{matrix}\n",
    "$\n",
    "\n",
    "In Python, we can perform this calculation using a single command as:\n",
    "\n",
    "```python\n",
    "Z = np.dot(w.T, X) + b\n",
    "```\n",
    "\n",
    "Where `b` can be a $\\mathbb{R}^{1}$ with numpy library performing \"broadcast\" of the elements. Stacking elements $a^{(1)}$, $a^{(2)}$, ... $a^{(m)}$ into a single row vector, we obtain the vector `A` as:\n",
    "\n",
    "$\n",
    "A = \\begin{bmatrix}\n",
    "a^{(1)} & a^{(2)} & \\ldots & a^{(m)}\n",
    "\\end{bmatrix} \\hspace{15pt}\\rightarrow\\hspace{15pt} (1, m)\\hspace{15pt} \\mathbb{R}^{1,m}\n",
    "$\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Vectorizing Logistic Regression's Gradient Output\n",
    "\n",
    "So, for the gradient computation, we computed $dz^{(1)}$ for the first example, which is $a^{(1)} - y^1$, $dz^{(2)}$ which is equal to $a^{(2)} - y^2$ and so on for all `m` training examples.\n",
    "\n",
    "$$\n",
    "\\begin{matrix}\n",
    "dz^{(1)} = a^{(1)} - y^1 & \\ \\ \\ \\ \\ & dz^{(2)} = a^{(2)} - y^2 & \\ \\ \\ \\ \\ & dz^{(3)} = a^{(3)} - y^3 \\\\ \n",
    "\\end{matrix}\n",
    "$$\n",
    "\n",
    "When vectorizing, we define a new variable `dZ` that contains $dz^{(1)}$, $dz^{(2)}$, up to $dz^{(m)}$ as illustred below. \n",
    "\n",
    "$$\n",
    "dZ = \\begin{bmatrix}\n",
    "dz^{(1)} & dz^{(2)} & \\ldots & dz^{(m)} \\\\ \n",
    "\\end{bmatrix}\n",
    "$$\n",
    "\n",
    "Note that, all the `dz` variables are stacked horizontally, generating a ($1, m$) matrix or alternatively a $\\mathbb{R}^{1,m}$ dimensional row vector. We know that `A` and `Y` are also matrices with variables stacked horizontally as:\n",
    "\n",
    "$$\n",
    "A = \\begin{bmatrix}\n",
    "a^{(1)} & a^{(2)} & \\ldots & a^{(m)} \\\\ \n",
    "\\end{bmatrix} \\\\\n",
    "Y = \\begin{bmatrix}\n",
    "y^{(1)} & y^{(2)} & \\ldots & y^{(m)} \\\\ \n",
    "\\end{bmatrix}\n",
    "$$\n",
    "\n",
    "Based on these definitions, we can compute `dZ` as `A` minus `Y` because it's going to be equal to: \n",
    "\n",
    "$$\n",
    "dZ = A - Y \\\\\n",
    "dZ = \\begin{bmatrix}\n",
    "a^{(1)} - y^{(1)} & a^{(2)} - y^{(2)} & \\ldots & a^{(m)} - y^{(m)} \\\\ \n",
    "\\end{bmatrix}\n",
    "$$\n",
    "\n",
    "In order to compute `db`, we sum all elements of `dz` and divide by the number of elements `m` as:\n",
    "\n",
    "$$\n",
    "db = \\frac{1}{m} \\sum_{i=1}^{m} dz^{(i)}\n",
    "$$\n",
    "\n",
    "In a vectorized implementation using python, we can run:\n",
    "\n",
    "```python\n",
    "db = 1./m * np.sum(dZ)\n",
    "```\n",
    "\n",
    "Finally, for computing `dw` in non-vectorized version, we have:\n",
    "\n",
    "$\n",
    "dw = 0 \\\\\n",
    "dw\\ += x^{(1)} dz^{(1)} \\\\\n",
    "dw\\ += x^{(2)} dz^{(2)} \\\\\n",
    "\\ldots \\\\\n",
    "dw\\ += x^{(m)} dz^{(m)} \\\\\n",
    "dw\\ /= m\n",
    "$\n",
    "\n",
    "On the other hand, with a vectorized implementation, we have:\n",
    "\n",
    "$$\n",
    "dw = \\frac{1}{m} X dZ^T\n",
    "$$\n",
    "\n",
    "Representing each matrix of the equation, we have:\n",
    "\n",
    "$$\n",
    "dw = \\frac{1}{m} \\begin{bmatrix}\n",
    "| & | &  & |\\\\\n",
    "x^{(1)} & x^{(2)} & \\dots & x^{(m)} \\\\\n",
    "| & | &  & |\\\\\n",
    "\\end{bmatrix} \\begin{bmatrix}\n",
    "dz^{(1)} \\\\\n",
    "dz^{(2)} \\\\\n",
    "\\ldots \\\\\n",
    "dz^{(m)}\n",
    "\\end{bmatrix} = \\frac{1}{m} \\begin{bmatrix}\n",
    "x^{(1)} dz^{(1)} & x^{(2)} dz^{(2)} & \\ldots & x^{(m)} dz^{(m)} \\\\\n",
    "\\end{bmatrix}\n",
    "$$\n",
    "\n",
    "which generates a ($1, m$) matrix. Finally, our algorithm to compute the derivatives becomes:\n",
    "\n",
    "$$\n",
    "\\begin{matrix}\n",
    "Z = w^{T}X + b & \\text{# np.dot(w.T, X) + b} \\\\\n",
    "A = \\sigma(Z)  & \\\\\n",
    "dZ = A - Y & \\\\\n",
    "dw = \\frac{1}{m} X dZ^{T} & \\\\\n",
    "db = \\frac{1}{m} \\text{np.sum(dZ)} & \\\\\n",
    "\\end{matrix}\n",
    "$$\n",
    "\n",
    "And for update weights, we compute:\n",
    "\n",
    "$$\n",
    "w = w - \\alpha dw \\\\\n",
    "b = b - \\alpha db \\\\\n",
    "$$\n",
    "\n",
    "Below, there is an example of the Python implementation of this vectorized version:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 124,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Initialize elements\n",
    "def sigmoid(Z):\n",
    "    return 1./(1. + np.exp(-1.*Z))\n",
    "\n",
    "#hyperparameter\n",
    "lr = 0.1\n",
    "nb_epochs = 10\n",
    "# input (nx, m) matrix 3 features, 4 examples\n",
    "X = np.array([\n",
    "    [0.1, 0.1, 0.1, 0.1],\n",
    "    [0.2, 0.2, 0.2, 0.2],\n",
    "    [0.3, 0.3, 0.3, 0.3]\n",
    "])\n",
    "n, m = X.shape\n",
    "Y = np.array([1, 2, 0, 2])\n",
    "\n",
    "# weights (1, n) matrix\n",
    "w = np.array([0.02, 0.03, 0.01])\n",
    "dw = np.zeros((1, m))\n",
    "# bias\n",
    "b = np.array([1.0])\n",
    "db = 0\n",
    "\n",
    "Z = np.zeros((1, m))\n",
    "dZ = np.zeros((1, m))\n",
    "A = np.zeros((1, m))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 125,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Error: 0.718222810093\n",
      "Error: 0.709879917577\n",
      "Error: 0.702678351072\n",
      "Error: 0.696521727895\n",
      "Error: 0.691322332225\n",
      "Error: 0.687000366625\n",
      "Error: 0.683483249913\n",
      "Error: 0.680704964154\n",
      "Error: 0.678605451445\n",
      "Error: 0.677130059769\n"
     ]
    }
   ],
   "source": [
    "for i in range(nb_epochs):\n",
    "    Z = np.dot(w.T, X) + b\n",
    "    A = sigmoid(Z)\n",
    "    dZ = A - Y\n",
    "    dw = 1./m * np.dot(X, dZ.T)\n",
    "    db = 1./m * np.sum(dZ)\n",
    "    J = 1./m * (- np.dot(Y, np.log(A)) + np.dot((1 - Y), np.log(1 - A)))\n",
    "    \n",
    "    w = w - lr*dw\n",
    "    b = b - lr*db\n",
    "    print(\"Error: {}\".format(J))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Broadcasting in Python\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "# Questions"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "1. Vectorization cannot be done without a GPU.<br>\n",
    "\n",
    "&#9745; False<br>\n",
    "&#9744; True"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "2. What are the dimensions of matrix `X` in this video?<br>\n",
    "\n",
    "&#9744; ($m, n_x$)<br>\n",
    "&#9745; ($n_x, m$)<br>\n",
    "&#9744; ($m, $)<br>\n",
    "&#9744; ($m, 1$)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "3. How do you compute the derivative of `b` in one line of code in Python numpy?<br>\n",
    "\n",
    "&#9745; `1 / m*(np.sum(dz))`<br>\n",
    "&#9744; `1 - m(np.sum(dz))`<br>\n",
    "&#9744; `m(np.sum(dz))`<br>\n",
    "&#9744; `1 * m(np.sum(dz))`<br>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "&#9744; "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 96,
   "metadata": {},
   "outputs": [
    {
     "ename": "ValueError",
     "evalue": "non-broadcastable output operand with shape (2,1) doesn't match the broadcast shape (2,2)",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mValueError\u001b[0m                                Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-96-38039b849f9c>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m     24\u001b[0m     \u001b[0mJ\u001b[0m \u001b[0;34m+=\u001b[0m \u001b[0;34m-\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0my\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mi\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m*\u001b[0m \u001b[0mmath\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mlog\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0ma\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mi\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m+\u001b[0m \u001b[0;34m(\u001b[0m\u001b[0;36m1\u001b[0m \u001b[0;34m-\u001b[0m \u001b[0my\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mi\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m*\u001b[0m \u001b[0mmath\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mlog\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;36m1\u001b[0m \u001b[0;34m-\u001b[0m \u001b[0ma\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mi\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     25\u001b[0m     \u001b[0mdz\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mi\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0ma\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mi\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m-\u001b[0m \u001b[0my\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mi\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 26\u001b[0;31m     \u001b[0mdw\u001b[0m \u001b[0;34m+=\u001b[0m \u001b[0mdz\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mi\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m*\u001b[0m \u001b[0mx\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mi\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     27\u001b[0m     \u001b[0mdb\u001b[0m \u001b[0;34m+=\u001b[0m \u001b[0mdz\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mi\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     28\u001b[0m \u001b[0mdb\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mValueError\u001b[0m: non-broadcastable output operand with shape (2,1) doesn't match the broadcast shape (2,2)"
     ]
    }
   ],
   "source": [
    "x = np.array([\n",
    "    [0.1, 0.1],\n",
    "    [0.2, 0.2],\n",
    "    [0.3, 0.3]\n",
    "])\n",
    "m = x.shape[0]\n",
    "w = np.array([0.2,0.3])\n",
    "b = [1.0, 1.0]\n",
    "y = np.array([1, 2, 3])\n",
    "z = np.zeros((m, 1))\n",
    "a = np.zeros((m, 1))\n",
    "dz = np.zeros((m, 1))\n",
    "w = np.array([0.2, 0.3])\n",
    "n = w.shape[0]\n",
    "dw = np.zeros((n, 1)) \n",
    "b = np.array([1.0])\n",
    "db = np.zeros(b.shape)\n",
    "\n",
    "J = 0\n",
    "\n",
    "for i in range(x.shape[0]):\n",
    "    z[i] = np.dot(w, x[i]) + b\n",
    "    a[i] = sigmoid(z[i])\n",
    "    J += -(y[i] * math.log(a[i]) + (1 - y[i]) * math.log(1 - a[i]))\n",
    "    dz[i] = a[i] - y[i]\n",
    "    dw += dz[i] * x[i]\n",
    "    db += dz[i]\n",
    "db"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 108,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[0.1, 0.1],\n",
       "       [0.2, 0.2],\n",
       "       [0.3, 0.3]])"
      ]
     },
     "execution_count": 108,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "x.T"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 110,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([0.1, 0.2, 0.3])"
      ]
     },
     "execution_count": 110,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "x.T[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "J, dw1, dw2, db = 0.0, 0.0, 0.0, 0.0\n",
    "for i in range(x.shape[0]):\n",
    "    z[i] = np.dot(x[i], w) + b\n",
    "    a[i] = sigmoid(z[i])\n",
    "    J += -(y[i] * math.log(a[i]) + (1 - y[i]) * math.log(1 - a[i]))\n",
    "    dz[i] = a[i] - y[i]\n",
    "    print x.shape\n",
    "    print dz.shape\n",
    "    dw += x.T[i] * dz[i]\n",
    "    db += dz[i]\n",
    "J /= m\n",
    "dw /= m\n",
    "db /= m\n",
    "    \n",
    "w = w - lr * dw\n",
    "b = b - lr * db\n",
    "\n",
    "print('Initial error: 0.0')\n",
    "print('Error: {}'.format(J))\n",
    "print('Initial weights: [0.2, 0.3]')\n",
    "print('Updated weights: {}'.format(w))"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 2",
   "language": "python",
   "name": "python2"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
