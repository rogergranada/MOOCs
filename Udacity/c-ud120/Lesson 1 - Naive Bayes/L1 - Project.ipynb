{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "\"\"\"\n",
    "A couple of years ago, J.K. Rowling (of Harry Potter fame) tried something interesting. She wrote a book, \"The Cuckoo's Calling,\" \n",
    "under the name Robert Galbraith. The book received some good reviews, but no one paid much attention to it--until an anonymous \n",
    "tipster on Twitter said it was J.K. Rowling. The London Sunday Times enlisted two experts to compare the linguistic patterns of \n",
    "\"Cuckoo\" to Rowling's \"The Casual Vacancy,\" as well as to books by several other authors. After the results of their analysis \n",
    "pointed strongly toward Rowling as the author, the Times directly asked the publisher if they were the same person, and the \n",
    "publisher confirmed. The book exploded in popularity overnight.\n",
    "\n",
    "We'll do something very similar in this project. We have a set of emails, half of which were written by one person and the other \n",
    "half by another person at the same company. Our objective is to classify the emails as written by one person or the other based \n",
    "only on the text of the email. We will start with Naive Bayes in this mini-project, and then expand in later projects to other \n",
    "algorithms.\n",
    "\n",
    "We will start by giving you a list of strings. Each string is the text of an email, which has undergone some basic preprocessing; \n",
    "we will then provide the code to split the dataset into training and testing sets. (In the next lessons you'll learn how to do this \n",
    "preprocessing and splitting yourself, but for now we'll give the code to you).\n",
    "\n",
    "One particular feature of Naive Bayes is that it's a good algorithm for working with text classification. When dealing with text, \n",
    "it's very common to treat each unique word as a feature, and since the typical person's vocabulary is many thousands of words, \n",
    "this makes for a large number of features. The relative simplicity of the algorithm and the independent features assumption of \n",
    "Naive Bayes make it a strong performer for classifying texts. In this mini-project, you will download and install sklearn on your \n",
    "computer and use Naive Bayes to classify emails by author.\n",
    "\"\"\"\n",
    "\n",
    "#!/usr/bin/python\n",
    "\n",
    "import pickle\n",
    "import cPickle\n",
    "import numpy\n",
    "\n",
    "from sklearn import cross_validation\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "from sklearn.feature_selection import SelectPercentile, f_classif\n",
    "\n",
    "\n",
    "def preprocess(words_file = \"word_data.pkl\", authors_file=\"email_authors.pkl\"):\n",
    "    \"\"\" \n",
    "        this function takes a pre-made list of email texts (by default word_data.pkl)\n",
    "        and the corresponding authors (by default email_authors.pkl) and performs\n",
    "        a number of preprocessing steps:\n",
    "            -- splits into training/testing sets (10% testing)\n",
    "            -- vectorizes into tfidf matrix\n",
    "            -- selects/keeps most helpful features\n",
    "\n",
    "        after this, the feaures and labels are put into numpy arrays, which play nice with sklearn functions\n",
    "\n",
    "        4 objects are returned:\n",
    "            -- training/testing features\n",
    "            -- training/testing labels\n",
    "\n",
    "    \"\"\"\n",
    "\n",
    "    ### the words (features) and authors (labels), already largely preprocessed\n",
    "    ### this preprocessing will be repeated in the text learning mini-project\n",
    "    authors_file_handler = open(authors_file, \"r\")\n",
    "    authors = pickle.load(authors_file_handler)\n",
    "    authors_file_handler.close()\n",
    "\n",
    "    words_file_handler = open(words_file, \"r\")\n",
    "    word_data = cPickle.load(words_file_handler)\n",
    "    words_file_handler.close()\n",
    "\n",
    "    ### test_size is the percentage of events assigned to the test set\n",
    "    ### (remainder go into training)\n",
    "    features_train, features_test, labels_train, labels_test = cross_validation.train_test_split(word_data, authors, test_size=0.1, random_state=42)\n",
    "\n",
    "\n",
    "\n",
    "    ### text vectorization--go from strings to lists of numbers\n",
    "    vectorizer = TfidfVectorizer(sublinear_tf=True, max_df=0.5,\n",
    "                                 stop_words='english')\n",
    "    features_train_transformed = vectorizer.fit_transform(features_train)\n",
    "    features_test_transformed  = vectorizer.transform(features_test)\n",
    "\n",
    "\n",
    "\n",
    "    ### feature selection, because text is super high dimensional and \n",
    "    ### can be really computationally chewy as a result\n",
    "    selector = SelectPercentile(f_classif, percentile=10)\n",
    "    selector.fit(features_train_transformed, labels_train)\n",
    "    features_train_transformed = selector.transform(features_train_transformed).toarray()\n",
    "    features_test_transformed  = selector.transform(features_test_transformed).toarray()\n",
    "\n",
    "    ### info on the data\n",
    "    print \"no. of Chris training emails:\", sum(labels_train)\n",
    "    print \"no. of Sara training emails:\", len(labels_train)-sum(labels_train)\n",
    "    \n",
    "    return features_train_transformed, features_test_transformed, labels_train, labels_test"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "no. of Chris training emails: 7936\n",
      "no. of Sara training emails: 7884\n"
     ]
    }
   ],
   "source": [
    "#!/usr/bin/python\n",
    "\n",
    "\"\"\" \n",
    "    This is the code to accompany the Lesson 1 (Naive Bayes) mini-project. \n",
    "\n",
    "    Use a Naive Bayes Classifier to identify emails by their authors\n",
    "    \n",
    "    authors and labels:\n",
    "    Sara has label 0\n",
    "    Chris has label 1\n",
    "\"\"\"\n",
    "    \n",
    "import sys\n",
    "from time import time\n",
    "#sys.path.append(\"../tools/\")\n",
    "#from email_preprocess import preprocess\n",
    "\n",
    "\n",
    "### features_train and features_test are the features for the training\n",
    "### and testing datasets, respectively\n",
    "### labels_train and labels_test are the corresponding item labels\n",
    "features_train, features_test, labels_train, labels_test = preprocess()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy: 0.973265073948\n"
     ]
    }
   ],
   "source": [
    "\"\"\"\n",
    "Create and train a Naive Bayes classifier in naive_bayes/nb_author_id.py. Use it to make predictions for the test set. \n",
    "What is the accuracy?\n",
    "\n",
    "When training you may see the following error: UserWarning: Duplicate scores. Result may depend on feature ordering. There are \n",
    "probably duplicate features, or you used a classification score for a regression task. warn(\"Duplicate scores. Result may depend \n",
    "on feature ordering.\")\n",
    "\n",
    "This is a warning that two or more words happen to have the same usage patterns in the emails--as far as the algorithm is \n",
    "concerned, this means that two features are the same. Some algorithms will actually break (mathematically won't work) or give \n",
    "multiple different answers (depending on feature ordering) when there are duplicate features and sklearn is giving us a warning. \n",
    "Good information, but not something we have to worry about.\n",
    "\"\"\"\n",
    "from sklearn.naive_bayes import GaussianNB\n",
    "from sklearn.metrics import accuracy_score\n",
    "\n",
    "### create classifier\n",
    "clf = GaussianNB()\n",
    "\n",
    "### fit the classifier on the training features and labels\n",
    "clf.fit(features_train, labels_train)\n",
    "\n",
    "### use the trained classifier to predict labels for the test features\n",
    "pred = clf.predict(features_test)\n",
    "\n",
    "### calculate and return the accuracy on the test data\n",
    "### this is slightly different than the example, \n",
    "### where we just print the accuracy\n",
    "### you might need to import an sklearn module\n",
    "accuracy = accuracy_score(pred, labels_test)\n",
    "print 'Accuracy:', accuracy\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "training time: 1.254 s\n",
      "predicting time: 0.189 s\n"
     ]
    }
   ],
   "source": [
    "\"\"\"\n",
    "An important topic that we didn't explicitly talk about is the time to train and test our algorithms. Put in two lines of code, \n",
    "above and below the line fitting your classifier, like this:\n",
    "\n",
    "t0 = time()\n",
    "< your clf.fit() line of code >\n",
    "print \"training time:\", round(time()-t0, 3), \"s\"\n",
    "\n",
    "Put similar lines of code around the clf.predict() line of code, so you can compare the time to train the classifier and to make \n",
    "predictions with it. What is faster, training or prediction?\n",
    "\n",
    "We will compare the Naive Bayes timing to a couple other algorithms, so note down the speed and accuracy you get and weâ€™ll revisit \n",
    "this in the next mini-project.\n",
    "\"\"\"\n",
    "clf = GaussianNB()\n",
    "\n",
    "t0 = time()\n",
    "clf.fit(features_train, labels_train)\n",
    "print \"training time:\", round(time()-t0, 3), \"s\"\n",
    "\n",
    "t0 = time()\n",
    "pred = clf.predict(features_test)\n",
    "print \"predicting time:\", round(time()-t0, 3), \"s\"\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 2",
   "language": "python",
   "name": "python2"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
