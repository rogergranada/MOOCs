{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Decision Trees\n",
    "\n",
    "A decision tree is a flowchart-like structure in which each internal node represents a \"test\" on an attribute (e.g. whether a coin flip comes up heads or tails), each branch represents the outcome of the test, and each leaf node represents a class label (decision taken after computing all attributes). The paths from root to leaf represent classification rules.\n",
    "\n",
    "In the example below, we can fill a table containing all elements of the decision tree. For instance, considering part of the tree as the image below, we can fill the table as:\n",
    "\n",
    "<img src=\"https://cdn.jsdelivr.net/gh/rogergranada/MOOCs/edX/GTx/CS7641/SL1/images/decision_tree.svg\" align=\"center\" width=\"300\"/>\n",
    "    \n",
    "| Occupied | Type  | Rainy | Hungry | Hot | Date | Happiness | Class  |\n",
    "| :------: | :---: | :---: | :----: | :-: | :--: | :-------: | :----: |\n",
    "|    T     | Pizza |   T   |   T    |  T  |  T   |     T     | go     |\n",
    "|    T     | Thai  |   T   |   T    |  T  |  T   |     F     | not go |\n",
    "|    T     | Thai  |   T   |   T    |  T  |  F   |     T     | not go |\n",
    "|    T     | Other |   F   |   T    |  T  |  T   |     F     | not go |\n",
    "|    T     | Other |   F   |   T    |  T  |  T   |     T     | not go |\n",
    "  \n",
    "\n",
    "Quiz 1: Which is the best division of data in a decision tree?\n",
    "\n",
    "<img src=\"https://cdn.jsdelivr.net/gh/rogergranada/MOOCs/edX/GTx/CS7641/SL1/images/quiz_1.svg\" align=\"center\" width=\"600\"/>\n",
    "\n",
    "The best division is the one that minimizes the entropy. Thus, the division presented in the center is the best one since it can divide balls and crosses in two different groups."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Decision Trees Expressiveness AND\n",
    "\n",
    "Create a decision tree that represents the boolean expression **A AND B**:\n",
    "\n",
    "<img src=\"https://cdn.jsdelivr.net/gh/rogergranada/MOOCs/edX/GTx/CS7641/SL1/images/decision_and.svg\" align=\"center\" width=\"400\"/>\n",
    "\n",
    "Create a decision tree that represents the boolean expression **A OR B**:\n",
    "\n",
    "<img src=\"https://cdn.jsdelivr.net/gh/rogergranada/MOOCs/edX/GTx/CS7641/SL1/images/decision_or.svg\" align=\"center\" width=\"400\"/>\n",
    "\n",
    "Create a decision tree that represents the boolean expression **A XOR B**:\n",
    "\n",
    "<img src=\"https://cdn.jsdelivr.net/gh/rogergranada/MOOCs/edX/GTx/CS7641/SL1/images/decision_xor.svg\" align=\"center\" width=\"200\"/>\n",
    "\n",
    "As we can see, for the XOR expression, we have to build the entire decision tree instead of having a pruned version of it as occurs in AND and OR expressions. In case of we have a decision tree with more attributes, the number of nodes grows fast. For example, generalizing the **OR** function to more than two attributes, we have the **N-OR** expression, also called the **ANY** expression. The decision tree for an **ANY** expression is:\n",
    "\n",
    "<img src=\"https://cdn.jsdelivr.net/gh/rogergranada/MOOCs/edX/GTx/CS7641/SL1/images/decision_any.svg\" align=\"center\" width=\"150\"/>\n",
    "\n",
    "In the case of an **N-OR** expression, we need $n$ nodes in the tree and the complexity is linear. Now, if we want to build an **N-XOR** decision tree, in an *odd parity*, i.e., if the number of TRUE in the same branch of the tree is odd, then the output is also TRUE, we have:\n",
    "\n",
    "<img src=\"https://cdn.jsdelivr.net/gh/rogergranada/MOOCs/edX/GTx/CS7641/SL1/images/decision_nxor.svg\" align=\"center\" width=\"250\"/>\n",
    "\n",
    "As we can see, the **N-XOR** decision tree needs an exponensial number of nodes as it grows, having a complexity of $O(2^n)$."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Decision trees: Expressiveness\n",
    "\n",
    "- XOR is hard  \n",
    "- $n$ atributes (boolean) O(n!)\n",
    "- How many trees? (a lot!)\n",
    "- Output is boolean\n",
    "\n",
    "**Truth table**:\n",
    "\n",
    "| $A_1$ | $A_2$ | $A_3$ | ... | $A_n$ | Output |\n",
    "| ----- | ----- | ----- | --- | ----- | ------ |\n",
    "|   T   |   T   |   T   | ... |   T   |   T/F  |\n",
    "|   T   |   T   |   T   | ... |   F   |   T/F  |\n",
    "|   F   |   T   |   T   | ... |   T   |   T/F  | \n",
    "|  ...  |  ...  |  ...  | ... |  ...  |        |\n",
    "|   F   |   F   |   F   | ... |   F   |   T/F  |\n",
    "\n",
    "- How many rows? $2^n$\n",
    "- How big is the truth table, i.e, how many ways to fill the output? $2^{2^n}$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Size of truth table for n=1: 4\n",
      "Size of truth table for n=2: 16\n",
      "Size of truth table for n=3: 256\n",
      "Size of truth table for n=4: 65536\n",
      "Size of truth table for n=5: 4294967296\n",
      "Size of truth table for n=6: 18446744073709551616\n",
      "Size of truth table for n=7: 340282366920938463463374607431768211456\n",
      "Size of truth table for n=8: 115792089237316195423570985008687907853269984665640564039457584007913129639936\n"
     ]
    }
   ],
   "source": [
    "# Checking how it grows\n",
    "def size_truth_table(n):\n",
    "    return 2**(2**n)\n",
    "\n",
    "for n in range(1,9):\n",
    "    print 'Size of truth table for n=%d: %d' % (n, size_truth_table(n))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## ID 3 Algorithm\n",
    "\n",
    "Loop:\n",
    " A <- best attribute # Information Gain(S, A)\n",
    " Assign A as decision attribute for *Node*\n",
    " For each value of A:\n",
    "  Create a descendant of *Node*\n",
    " Sort training examples to leaves\n",
    " If example perfectly classifies:\n",
    "  STOP\n",
    " Else:\n",
    "  Iterate over leaves\n",
    "\n",
    "$$Gain(S, A) = Entropy(S) - \\sum_v \\frac{|S_v|}{|S|}Entropy(S_v)$$\n",
    "\n",
    "where $S$ is the collection of examples and $A$ is an attribute, and Entropy is measured as:\n",
    "\n",
    "$$Entropy(S) = - \\sum_v p(v)\\ log\\ p(v)$$\n",
    "\n",
    "\n",
    "### ID3: Inductive bias\n",
    "\n",
    "- Prefer to select good splits at the top\n",
    "- Prefer correct ones instead of incorrect ones\n",
    "- Prefer short trees than longer trees"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Other considerations\n",
    "\n",
    "When dealing with continuous values, prefer to deal with ranges instead of values. For example, age could be divided into <20 and >=20. Thus, for continuous values we try to create Trues and Falses for ranges.\n",
    "\n",
    "The algorithm finishes when everything is correctly classified, or there are no more attributes to split"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 2",
   "language": "python",
   "name": "python2"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.15"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
